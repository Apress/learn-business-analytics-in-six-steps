# vector will have the most common / probable match first.
proposals_by_prob <- c(sorted_words[ edit_dist <= min(edit_dist, 2)])
# In case proposals_by_prob would be empty we append the word to be corrected...
proposals_by_prob <- c(proposals_by_prob, word)
# ... and return the first / most probable word in the vector.
proposals_by_prob[1]
}
correct
correct(sorted_words)
correct <- function(word) {
# Calculate the edit distance between the word and all other words in sorted_words.
edit_dist <- adist(word, sorted_words)
# Calculate the minimum edit distance to find a word that exists in big.txt
# with a limit of two edits.
min_edit_dist <- min(edit_dist, 2)
# Generate a vector with all words with this minimum edit distance.
# Since sorted_words is ordered from most common to least common, the resulting
# vector will have the most common / probable match first.
proposals_by_prob <- c(sorted_words[ edit_dist <= min(edit_dist, 2)])
# In case proposals_by_prob would be empty we append the word to be corrected...
proposals_by_prob <- c(proposals_by_prob, word)
}
final <- correct(sorted_words)
final
write.table(final, "F:/final.csv", sep=",")
raw_text <- read.csv("F:/Book4.csv", header=FALSE, stringsAsFactors=FALSE)
View(raw_text)
raw_text <- paste(raw_text$V1, collapse=" ")
# Make the text lowercase and split it up creating a huge vector of word tokens.
split_text <- strsplit(tolower(raw_text), "[^a-z]+")
# Count the number of different type of words.
word_count <- table(split_text)
# Sort the words and create an ordered vector with the most common type of words first.
sorted_words <- names(sort(word_count, decreasing = TRUE))
correct <- function(word) {
# Calculate the edit distance between the word and all other words in sorted_words.
edit_dist <- adist(word, sorted_words)
# Calculate the minimum edit distance to find a word that exists in big.txt
# with a limit of two edits.
min_edit_dist <- min(edit_dist, 2)
# Generate a vector with all words with this minimum edit distance.
# Since sorted_words is ordered from most common to least common, the resulting
# vector will have the most common / probable match first.
proposals_by_prob <- c(sorted_words[ edit_dist <= min(edit_dist, 2)])
# In case proposals_by_prob would be empty we append the word to be corrected...
proposals_by_prob <- c(proposals_by_prob, word)
}
final <- correct(sorted_words)
final
write.table(final, "F:/final.csv", sep=",")
raw_text <- read.csv("F:/Book3.csv", header=FALSE, stringsAsFactors=FALSE)
View(raw_text)
raw_text <- paste(raw_text$V1, collapse=" ")
# Make the text lowercase and split it up creating a huge vector of word tokens.
split_text <- strsplit(tolower(raw_text), "[^a-z]+")
# Count the number of different type of words.
word_count <- table(split_text)
# Sort the words and create an ordered vector with the most common type of words first.
sorted_words <- names(sort(word_count, decreasing = TRUE))
correct <- function(word) {
# Calculate the edit distance between the word and all other words in sorted_words.
edit_dist <- adist(word, sorted_words)
# Calculate the minimum edit distance to find a word that exists in big.txt
# with a limit of two edits.
min_edit_dist <- min(edit_dist, 2)
# Generate a vector with all words with this minimum edit distance.
# Since sorted_words is ordered from most common to least common, the resulting
# vector will have the most common / probable match first.
proposals_by_prob <- c(sorted_words[ edit_dist <= min(edit_dist, 2)])
# In case proposals_by_prob would be empty we append the word to be corrected...
proposals_by_prob <- c(proposals_by_prob, word)
}
final <- correct(sorted_words)
final
write.table(final, "F:/final3.csv", sep=",")
review_text <- paste(final, collapse=" ")
View(review_text)
review_source <- VectorSource(review_text)
corpus=Corpus(review_text)
corpus <- tm_map(review_text, content_transformer(tolower))
llibrary(tm)
library(tm)
corpus<- Corpus(review_text)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(review_text, content_transformer(tolower))
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc")
install.packages(Needed, dependencies=TRUE)
docs <- read.csv("F:/Book3.csv", header=FALSE, stringsAsFactors=FALSE)
docs <- tm_map(docs, removePunctuation)
View(docs)
docs <- paste(docs$V1, collapse=" ")
docs1 <-  Corpus(VectorSource(docs))
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, removeNumbers)
str(docs)
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
check_spelling <- function(text.var, range = 2,
assume.first.correct = TRUE, method = "jw",
dictionary = qdapDictionaries::GradyAugmented, parallel = TRUE,
cores = parallel::detectCores()/2, n.suggests = 8) {
dictnchar <- nchar(dictionary)
dictfirst <- substring(dictionary, 1, 1)
if (parallel && cores > 1){
cl <- parallel::makeCluster(mc <- getOption("cl.cores", cores))
vars <- c("text.var", "which_misspelled", "dictnchar",
"dictfirst", "bag_o_words", "range", "mgsub", "method",
"n.suggests")
parallel::clusterExport(cl=cl, varlist=vars, envir = environment())
check_spelling <- function(text.var, range = 2,
assume.first.correct = TRUE, method = "jw",
dictionary = qdapDictionaries::GradyAugmented, parallel = TRUE,
cores = parallel::detectCores()/2, n.suggests = 8) {
dictnchar <- nchar(dictionary)
dictfirst <- substring(dictionary, 1, 1)
if (parallel && cores > 1){
cl <- parallel::makeCluster(mc <- getOption("cl.cores", cores))
vars <- c("text.var", "which_misspelled", "dictnchar",
"dictfirst", "bag_o_words", "range", "mgsub", "method",
"n.suggests")
parallel::clusterExport(cl=cl, varlist=vars, envir = environment())
out <- stats::setNames(parallel::parLapply(cl, text.var, which_misspelled,
range = range, suggest = TRUE, method = method,
assume.first.correct = assume.first.correct, n.suggests = n.suggests,
dictionary = dictionary, nchar.dictionary = dictnchar,
first.char.dictionary=dictfirst), 1:length(text.var))
parallel::stopCluster(cl)
} else {
out <- stats::setNames(lapply(text.var, which_misspelled, range = range,
suggest = TRUE,  method = method, n.suggests = n.suggests,
assume.first.correct = assume.first.correct,
dictionary = dictionary, nchar.dictionary = dictnchar,
first.char.dictionary=dictfirst), 1:length(text.var))
}
out <- out[!sapply(out, is.null)]
if (identical(unname(out), list())) {
message("No spelling errors detected. Spell check complete.")
return(invisible(NULL))
}
out <- docs(row = rep(as.numeric(names(out)), sapply(out, nrow)),
do.call(rbind, out), stringsAsFactors = FALSE, row.names = NULL)
class(out)  <- c("check_spelling", "which_misspelled", class(out))
attributes(out)[["text.var"]] <- text.var
out
}
Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc")
install.packages(Needed, dependencies=TRUE)
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
docs <- read.csv("F:/Book3.csv", header=FALSE, stringsAsFactors=FALSE)
View(docs)
docs <- paste(docs$V1, collapse=" ")
str(docs)
install.packages("utils")
check_spelling <- function(text.var, range = 2,
assume.first.correct = TRUE, method = "jw",
dictionary = qdapDictionaries::GradyAugmented, parallel = TRUE,
cores = parallel::detectCores()/2, n.suggests = 8) {
dictnchar <- nchar(dictionary)
dictfirst <- substring(dictionary, 1, 1)
if (parallel && cores > 1){
cl <- parallel::makeCluster(mc <- getOption("cl.cores", cores))
vars <- c("text.var", "which_misspelled", "dictnchar",
"dictfirst", "bag_o_words", "range", "mgsub", "method",
"n.suggests")
parallel::clusterExport(cl=cl, varlist=vars, envir = environment())
out <- stats::setNames(parallel::parLapply(cl, text.var, which_misspelled,
range = range, suggest = TRUE, method = method,
assume.first.correct = assume.first.correct, n.suggests = n.suggests,
dictionary = dictionary, nchar.dictionary = dictnchar,
first.char.dictionary=dictfirst), 1:length(text.var))
parallel::stopCluster(cl)
} else {
out <- stats::setNames(lapply(text.var, which_misspelled, range = range,
suggest = TRUE,  method = method, n.suggests = n.suggests,
assume.first.correct = assume.first.correct,
dictionary = dictionary, nchar.dictionary = dictnchar,
first.char.dictionary=dictfirst), 1:length(text.var))
}
out <- out[!sapply(out, is.null)]
if (identical(unname(out), list())) {
message("No spelling errors detected. Spell check complete.")
return(invisible(NULL))
}
out <- docs(row = rep(as.numeric(names(out)), sapply(out, nrow)),
do.call(rbind, out), stringsAsFactors = FALSE, row.names = NULL)
class(out)  <- c("check_spelling", "which_misspelled", class(out))
attributes(out)[["text.var"]] <- text.var
out
}
which_misspelled <- function(x, suggest = FALSE, range = 2,
assume.first.correct = TRUE, dictionary = qdapDictionaries::GradyAugmented,
method = "jw", nchar.dictionary = nchar(dictionary),
first.char.dictionary = substring(dictionary, 1, 1), n.suggests = 8) {
if (length(x) > 1) stop("`x` must be a single string")
if (is.na(x)) return(NULL)
if (is.non.alpha(x)) return(NULL)
## break into a bag of words
wrds <- bag_o_words(x)
names(wrds) <- 1:length(wrds)
## determine if found in dictionary
misses <- wrds[is.na(match(mgsub(c("'d", "'s"), "", wrds), dictionary))]
if (identical(character(0), unname(misses))) return(NULL)
## if no suggestions desired retruns misspelled words
if (!suggest) return(misses)
## force eval of first.char.dictionary before
## possibly changing the dictionary
first.char.dictionary <- force(first.char.dictionary)
## combine the words from dictionary with numb. characters
dictionary <- docs(dictionary, nchar.dictionary,
stringsAsFactors = FALSE)
if(assume.first.correct && all(letters %in% sort(unique(first.char.dictionary)))) {
dictionary <- split(dictionary, first.char.dictionary)
}
replacements <- Map(function(x, y, dictionary2 = dictionary,
therange = range, n.sugg = n.suggests,
meth = method) {
## if assume first grab the dictionary list that
## starts with that letter
if(assume.first.correct) {
dictionary2 <- dictionary2[[substring(x, 1, 1)]]
}
## grab words form dictionary within that range
## of characters close to x
dict <- character(0)
while(identical(character(0), dict)) {
dict <- dictionary2[dictionary2[[2]] >= max(1, y - therange) & dictionary2[[2]] <= (y + therange), 1]
if (identical(character(0), dict)) {
message(paste("Range was not large enough.  Increasing range to",
therange + 1, "for: ", x))
therange <- therange + 1
utils::flush.console()
}
}
## find the disctionary word distance to the target ond sort
ratings <- sort(stats::setNames(stringdist::stringdist(x, dict, method = meth), dict))
## return the top n terms in order of likliness
names(ratings)[ratings <= ratings[min(n.sugg, length(ratings))]]
}, misses, nchar(misses))
out <- docs(word.no = names(misses), not.found=misses,
suggestion = unlist(lapply(replacements, "[", 1)),
stringsAsFactors = FALSE, row.names=NULL)
docs <- read.csv("F:/Book3.csv", header=FALSE, stringsAsFactors=FALSE)
words <- strip.text(docs)
words <- strip.text(docs)
words <- strip.text(docs)
docs <- read.csv("F:/Book3.csv", header=FALSE, stringsAsFactors=FALSE)
words <- strip.text(docs)
strip.text <- function(txt) {
# remove apostrophes (so "don't" -> "dont", "Jane's" -> "Janes", etc.)
txt <- gsub("'","",txt)
# convert to lowercase
txt <- tolower(txt)
# change other non-alphanumeric characters to spaces
txt <- gsub("[^a-z0-9]"," ",txt)
# change digits to #
txt <- gsub("[0-9]+"," ",txt)
# split and make one vector
txt <- unlist(strsplit(txt," "))
# remove empty words
txt <- txt[txt != ""]
return(txt)
}
words <- strip.text(docs)
counts <- table(words)
# Words within 1 transposition.
Transpositions <- function(word = FALSE) {
N <- nchar(word)
if (N > 2) {
out <- rep(word, N - 1)
word <- unlist(strsplit(word, NULL))
# Permutations of the letters
perms <- matrix(c(1:(N - 1), 2:N), ncol = 2)
reversed <- perms[, 2:1]
trans.words <- matrix(rep(word, N - 1), byrow = TRUE, nrow = N - 1)
for(i in 1:(N - 1)) {
trans.words[i, perms[i, ]] <- trans.words[i, reversed[i, ]]
out[i] <- paste(trans.words[i, ], collapse = "")
}
}
else if (N == 2) {
out <- paste(word[2:1], collapse = "")
}
else {
out <- paste(word, collapse = "")
}
return(out)
}
# Single letter deletions.
Deletes <- function(word = FALSE) {
N <- nchar(word)
word <- unlist(strsplit(word, NULL))
for(i in 1:N) {
out[i] <- paste(word[-i], collapse = "")
}
return(out)
}
# Single-letter insertions.
Insertions <- function(word = FALSE) {
N <- nchar(word)
out <- list()
for (letter in letters) {
out[[letter]] <- rep(word, N + 1)
for (i in 1:(N + 1)) {
out[[letter]][i] <- paste(substr(word, i - N, i - 1), letter,
substr(word, i, N), sep = "")
}
}
out <- unlist(out)
return(out)
}
# Single-letter replacements.
Replaces <- function(word = FALSE) {
N <- nchar(word)
out <- list()
for (letter in letters) {
out[[letter]] <- rep(word, N)
for (i in 1:N) {
out[[letter]][i] <- paste(substr(word, i - N, i - 1), letter,
substr(word, i + 1, N + 1), sep = "")
}
}
out <- unlist(out)
return(out)
}
# All Neighbors with distance "1"
Neighbors <- function(word) {
neighbors <- c(word, Replaces(word), Deletes(word),
Insertions(word), Transpositions(word))
return(neighbors)
}
# Probability as determined by our corpus.
Probability <- function(word, dtm) {
# Number of words, total
N <- length(dtm)
word.number <- which(names(dtm) == word)
count <- dtm[word.number]
pval <- count/N
return(pval)
}
# Correct a single word.
Correct <- function(word, dtm) {
neighbors <- Neighbors(word)
# If it is a word, just return it.
if (word %in% names(dtm)) {
out <- word
}
# Otherwise, check for neighbors.
else {
# Which of the neighbors are known words?
known <- which(neighbors %in% names(dtm))
N.known <- length(known)
# If there are no known neighbors, including the word,
# look farther away.
if (N.known == 0) {
print(paste("Having a hard time matching '", word, "'...", sep = ""))
neighbors <- unlist(lapply(neighbors, Neighbors))
}
# Then out non-words.
neighbors <- neighbors[which(neighbors %in% names(dtm))]
N <- length(neighbors)
# If we found some neighbors, find the one with the highest
# p-value.
if (N > 1) {
P <- 0*(1:N)
for (i in 1:N) {
P[i] <- Probability(neighbors[i], dtm)
}
out <- neighbors[which.max(P)]
}
# If no neighbors still, return the word.
else {
out <- word
}
}
return(out)
}
# Correct an entire document.
CorrectDocument <- function(document, dtm) {
by.word <- unlist(strsplit(document, " "))
N <- length(by.word)
for (i in 1:N) {
by.word[i] <- Correct(by.word[i], dtm = dtm)
}
corrected <- paste(by.word, collapse = " ")
return(corrected)
}
library("plyr")
<br />
<br />
source(&quot;https://raw.github.com/schaunwheeler/tmt/master/R/tmt.R&quot;)<br />
<br />
AspellCheck(input, output = c(&quot;eval&quot;, “sugg”, “fix”), sep = FALSE, cap.flag = c(&quot;none&quot;, “first”, “all”), ignore=NULL, split.missing = FALSE)<br />
AspellCheck(input, output = c(&quot;eval&quot;, “sugg”, “fix”), sep = FALSE, cap.flag = c(quot;none&quot;, “first”, “all”), ignore=NULL, split.missing = FALSE)<br />
<br />
AspellCheck(input, output = c(quot;eval&quot;, “sugg”, “fix”), sep = FALSE, cap.flag = c(quot;none&quot;, “first”, “all”), ignore=NULL, split.missing = FALSE)<br />
<br />
AspellCheck(input, output = c(quot eval&quot;, “sugg”, “fix”), sep = FALSE, cap.flag = c(quot none&quot;, “first”, “all”), ignore=NULL, split.missing = FALSE)<br />
library("plyr")
docs <- read.csv("F:/Book3.csv", header=FALSE, stringsAsFactors=FALSE)
docs <- paste(docs$V1, collapse=" ")
aspell(docs)
install.packages("Aspell", repos = "http://www.omegahat.org/R", type = "source")
wget ftp://ftp.gnu.org/gnu/aspell/dict/en/aspell6-en-7.1-0.tar.bz2
tar xvjf aspell6-en-7.1-0.tar.bz2
wget ftp://ftp.gnu.org/gnu/aspell/dict/en/aspell6-en-7.1-0.tar.bz2
tar xvjf aspell6-en-7.1-0.tar.bz2
cd aspell6-en-7.1-0/
./configure
make
make install
install.packages("aspell")
library(qdap)
check_spelling(docs$Var1, range = 2, assume.first.correct = TRUE,
method = "jw", dictionary = qdapDictionaries::GradyAugmented,
parallel = TRUE, cores = parallel::detectCores()/2, n.suggests = 8)
library(qdap)
check_spelling(docs$Var1, range = 2, assume.first.correct = TRUE,
method = "jw", dictionary = qdapDictionaries::GradyAugmented,
parallel = TRUE, cores = parallel::detectCores()/2, n.suggests = 8)
check_spelling(docs, range = 2, assume.first.correct = TRUE,
method = "jw", dictionary = qdapDictionaries::GradyAugmented,
parallel = TRUE, cores = parallel::detectCores()/2, n.suggests = 8)
list1<-check_spelling(docs, range = 2, assume.first.correct = TRUE,
method = "jw", dictionary = qdapDictionaries::GradyAugmented,
parallel = TRUE, cores = parallel::detectCores()/2, n.suggests = 8)
write.table(list1, "F:/list1.csv", sep=",")
write.table(list1, "F:/list1.txt", sep="\t")
View(list1)
View(list1)
docs <- read.csv("F:/Book4.csv", header=FALSE, stringsAsFactors=FALSE)
docs <- paste(docs$V1, collapse=" ")
install.packages("Aspell", repos = "http://www.omegahat.org/R", type = "source")
library(qdap)
list1<-check_spelling(docs, range = 2, assume.first.correct = TRUE,
method = "jw", dictionary = qdapDictionaries::GradyAugmented,
parallel = TRUE, cores = parallel::detectCores()/2, n.suggests = 8)
View(list1)
View(list1)
docs <- read.csv("F:/Book4.csv", header=FALSE, stringsAsFactors=FALSE)
docs <- paste(docs$V1, collapse=" ")
install.packages("Aspell", repos = "http://www.omegahat.org/R", type = "source")
library(qdap)
list1<-check_spelling(docs, range = 2, assume.first.correct = TRUE,
method = "jw", dictionary = qdapDictionaries::GradyAugmented,
parallel = TRUE, cores = parallel::detectCores()/2, n.suggests = 8)
View(list1)
library(xlsx)
write.xlsx(mydata, "c:/mydata.xlsx")
write.xlsx(list1, "F:/mydata.xlsx")
write.csv(list1, "F:/mydata.csv")
setwd("H:/springer book/Case study/CaseStudy5")
startup<- read.table("H:/springer book/Case study/CaseStudy5/startupcost.csv", header=TRUE, sep=",")
str(startup)
dim(startup)
> startup2<- startup[!complete.cases(startup[, c("X1", "X2","X3","X4","X5")]), ]
startup2<- startup[!complete.cases(startup[, c("X1", "X2","X3","X4","X5")]), ]
View(startup2)
View(startup)
var.test(X1, X2, ratio = 1, alternative = c("two.sided", "less", "greater"),
conf.level = 0.95, ...)
var.test(X1, X2, ratio = 1, alternative = c("two.sided", "less", "greater"),
conf.level = 0.95)
startup3<- startup[rowSums(is.na(startup))!=3, ]
startup3[is.na(startup3)] <- " "
View(startup3)
var.test(startup3$X1, startup3$X2, ratio = 1, alternative = c("two.sided", "less", "greater"),
conf.level = 0.95)
var.test(startup3$X1, startup3$X2, ratio = 1, alternative = c( "less", "greater"),
conf.level = 0.95)
var.test(startup3$X1, startup3$X2)
mysample <- startup3[sample(1:nrow(startup3), 50,
replace=FALSE),]
mysample <- startup3[sample(1:nrow(startup3), 50, replace=TRUE),]
dim(mysample)
var.test(mysample$X1, mysample$X2)
attach(mysample)
varr.test(X1,X2)
var.test(X1,X2)
mysample2<-na.omit(mysample)
attach(mysample2)
var.test(X1,X2)
var.test(mysample2$X1,mysample2$X2)
str(mysample2)
mysample2<-as.numeric(mysample2)
mysample3<- as.numeric(mysample2)
mysample2<-as.data.frame(mysample2)
str(mysample2)
mysample2=as.numeric(mysample2)
mysample2=as.numeric(unlist(mysample2))
mysample2<-na.omit(mysample2)
dim(mysample2)
startup<- as.numeric(startup)
startup<-as.numeric(unlist(startup))
startup<-na.omit(startup)
startup2<- startup[!complete.cases(startup[, c("X1", "X2","X3","X4","X5")]), ]
startup<- as.data.frame(startup)
